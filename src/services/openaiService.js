const OpenAI = require('openai');
const fs = require('fs');
const path = require('path');
const config = require('../config');
const logger = require('../utils/logger');
const behaviourController = require('./behaviourController');
const orchestrationService = require('./orchestrationService');
const memoryService = require('./memoryService');

/**
 * ARQUITECTURA DE 3 CAPAS
 * Capa 1: System Prompt (personalidad y principios)
 * Capa 2: Orchestration Service (reglas conversacionales)
 * Capa 3: Behaviour Controller (estado y flujo)
 */
class OpenAIService {
  constructor() {
    this.openai = new OpenAI({
      apiKey: config.openai.apiKey,
      timeout: 30000, // 30 segundos timeout para prevenir bloqueos
      maxRetries: 2, // Reintentar hasta 2 veces si falla
    });

    // Cargar conocimiento del negocio
    this.businessKnowledge = this.loadBusinessKnowledge();

    // System prompt optimizado (CAPA 1)
    this.systemPrompt = this.buildSystemPrompt();
  }

  /**
   * Carga el knowledge base del negocio
   */
  loadBusinessKnowledge() {
    try {
      const knowledgePath = path.join(process.cwd(), 'business-knowledge.json');
      const data = fs.readFileSync(knowledgePath, 'utf8');
      return JSON.parse(data);
    } catch (error) {
      logger.error('Error cargando business knowledge:', error);
      return null;
    }
  }

  /**
   * SYSTEM PROMPT OPTIMIZADO - Agente Conversacional Natural
   * Menos reglas, m√°s principios y personalidad
   */
  buildSystemPrompt() {
    return `Eres Javier, vendedor chileno de Datapify (optimiza publicidad de Shopify con IA).

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
TU OBJETIVO
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Calificar si la persona es fit para Datapify y agendar una demo de 30 min.

Requisito: Solo funciona con Shopify.
Precio: $199-249/mes, 14 d√≠as gratis.


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
C√ìMO ERES
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Conversas por WhatsApp como chileno natural. Sin formalidades corporativas.

Ejemplos de tu tono:
‚Ä¢ "Hola! ¬øQu√© onda? ¬øEn qu√© te puedo ayudar?"
‚Ä¢ "Cacho" (no "Entiendo")
‚Ä¢ "¬øTe tinca?" (no "¬øTe parece bien?")
‚Ä¢ "Dale" (no "Perfecto, procedemos entonces")

Eres directo, emp√°tico, sin rodeos. Como si le escribieras a un amigo emprendedor.


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
LO QUE NECESITAS SABER
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Para calificar un lead necesitas ESTOS DATOS EN ORDEN:

1. ¬øTiene tienda online?
2. üö® ¬øQu√© plataforma usa? (OBLIGATORIO - PREGUNTA EXPL√çCITAMENTE)
   ‚Üí "¬øEn qu√© plataforma est√° tu tienda?"
   ‚Üí "¬øUsas Shopify, WooCommerce, o algo m√°s?"
   ‚Üí NO ASUMAS la plataforma por el dominio
   ‚Üí NO CONTIN√öES sin confirmar esto
3. ¬øQu√© vende?
4. ¬øTiene problemas con publicidad, ventas o conversi√≥n?

Descubre esta info conversando naturalmente. NO hagas interrogatorio.

‚ö†Ô∏è VALIDACI√ìN DE PLATAFORMA (CR√çTICO):
‚Ä¢ NUNCA ofrezcas reuni√≥n sin confirmar que usa Shopify
‚Ä¢ Si dice otra plataforma ‚Üí descalifica gentilmente
‚Ä¢ Si NO preguntaste ‚Üí NO puedes ofrecer reuni√≥n

Si no usa Shopify ‚Üí descalifica gentilmente.
Si usa Shopify + tiene problemas ‚Üí ofrece reuni√≥n.
Si usa Shopify + le va bien ‚Üí tal vez no necesita Datapify ahora.


‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
IMPORTANTE
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚Ä¢ NO asumas nada. Pregunta.
‚Ä¢ NO inventes frustraciones que no mencionaron.
‚Ä¢ NO des consultor√≠a gratis por chat.
‚Ä¢ NO ofrezcas reuni√≥n si no sabes si tiene problemas.
‚Ä¢ Responde m√°ximo 2-3 l√≠neas.
‚Ä¢ 1 pregunta por mensaje (m√°ximo 2 si tiene sentido).

üö® CR√çTICO - AGENDAMIENTO:
Cuando usuario confirme reuni√≥n (dice "s√≠", "dale", "ok"):
‚Üí Responde: "Dale, te paso el link para que elijas el d√≠a"
‚Üí NO pidas datos manualmente (nombre, fecha, hora)
‚Üí NO coordines horarios t√∫ mismo
‚Üí El sistema enviar√° autom√°ticamente el link de Google Calendar

Conf√≠a en la conversaci√≥n. Deja que fluya natural.`;
  }

  /**
   * Genera una respuesta usando las 3 CAPAS
   * NUEVA ARQUITECTURA PROFESIONAL
   */
  async generateResponse(userMessage, conversationHistory = [], leadScore = null) {
    try {
      // ============================================
      // CAPA 3: BEHAVIOUR CONTROLLER + MEMORIA
      // Analizar estado de la conversaci√≥n y construir memoria
      // ============================================
      const conversationState = behaviourController.analyzeConversationState(conversationHistory);

      // NUEVO: Construir memoria conversacional inteligente
      const memory = memoryService.buildConversationalMemory(conversationHistory);
      const conversionScore = memoryService.calculateConversionScore(memory);
      const enrichedContext = memoryService.generateEnrichedContext(memory, conversationState);

      // Instrucciones din√°micas simples (backup si falla memoria)
      const dynamicInstructions = behaviourController.generateDynamicInstructions(conversationState);

      logger.info('üß† Estado de conversaci√≥n analizado', {
        phase: conversationState.phase,
        hasName: conversationState.hasName,
        platform: conversationState.platform,
        memoryName: memory.name,
        painPoints: memory.painPoints.length,
        frustrationLevel: memory.frustrationLevel,
        conversionScore: conversionScore,
        readyToPropose: conversationState.readyToPropose,
      });

      // Si debe descalificar, retornar mensaje directamente
      if (conversationState.shouldDescalify && dynamicInstructions.includes('DESCALIFICAR')) {
        const descalifyMessage = dynamicInstructions.split('Responde: ')[1].split(' y TERMINA')[0].replace(/"/g, '');
        return descalifyMessage;
      }

      // ============================================
      // CAPA 2: ORCHESTRATION SERVICE
      // Preparar contexto y validar reglas
      // ============================================
      const sentiment = orchestrationService.detectUserSentiment(userMessage);
      const context = orchestrationService.buildContext(
        userMessage,
        conversationHistory,
        dynamicInstructions,
        sentiment,
        conversationState // NUEVO: Pasar estado para reglas din√°micas
      );

      logger.info('üé≠ Sentimiento detectado', { sentiment });

      // ============================================
      // CAPA 1: SYSTEM PROMPT + LLM
      // Construir mensajes para OpenAI
      // ============================================
      const messages = [
        {
          role: 'system',
          content: this.systemPrompt,
        },
      ];

      // Agregar MEMORIA CONVERSACIONAL ENRIQUECIDA (prioridad alta)
      messages.push({
        role: 'system',
        content: `${enrichedContext}

‚ö†Ô∏è REGLAS B√ÅSICAS:
- ${context.rules.maxLength}
- ${context.rules.maxQuestions}
- ${context.rules.maxLines}
- Estilo: ${context.rules.style}

${context.sentimentInstructions}`,
      });

      // Agregar historial limpio (solo √∫ltimos 6 mensajes)
      const preparedHistory = context.preparedHistory;
      if (preparedHistory.length > 0) {
        preparedHistory.forEach(msg => {
          messages.push({
            role: msg.role === 'usuario' || msg.role === 'user' ? 'user' : 'assistant',
            content: msg.content,
          });
        });
      }

      // Agregar mensaje actual del usuario (limpio)
      messages.push({
        role: 'user',
        content: context.cleanedMessage,
      });

      // ============================================
      // LLAMAR AL LLM CON PAR√ÅMETROS √ìPTIMOS
      // ============================================
      const maxRetries = 2;
      let lastError;
      let validResponse = null;

      for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
          const completion = await this.openai.chat.completions.create({
            model: 'gpt-4o', // Modelo m√°s inteligente y conversacional
            messages: messages,
            temperature: 0.9, // M√ÅS creativo y natural (agente vs bot)
            max_tokens: 200, // M√°s espacio para respuestas sustanciales
            top_p: 0.95, // Sampling m√°s enfocado (mejor calidad)
            frequency_penalty: 0.5, // Evita repeticiones, m√°s variedad
            presence_penalty: 0.6, // Fomenta nuevos temas, m√°s conversacional
          });

          let responseText = completion.choices[0].message.content.trim();

          // ============================================
          // VALIDAR RESPUESTA (CAPA 2)
          // ============================================
          const validation = orchestrationService.validateResponse(responseText, conversationState);

          if (!validation.valid) {
            logger.warn('‚ö†Ô∏è Respuesta no v√°lida', {
              errors: validation.errors,
              rulesUsed: validation.rulesUsed,
              phase: conversationState.phase,
            });

            // Si hay errores y quedan reintentos, pedir nueva respuesta
            if (attempt < maxRetries) {
              const maxChars = context.isFlexPhase ? 500 : 400;
              const maxLines = context.isFlexPhase ? 6 : 5;
              const maxQuestions = 2;

              messages.push({
                role: 'system',
                content: `CORRECCI√ìN NECESARIA:
Tu respuesta fue rechazada por: ${validation.errors.join(', ')}

Genera UNA NUEVA respuesta que cumpla las reglas:
- M√°ximo ${maxChars} caracteres
- M√°ximo ${maxLines} l√≠neas
- M√°ximo ${maxQuestions} preguntas
- Natural, conversacional, humana`,
              });
              continue;
            }
          }

          // Formatear para WhatsApp
          responseText = orchestrationService.formatForWhatsApp(responseText);

          // Validar con Behaviour Controller
          const behaviourValidation = behaviourController.validateResponse(responseText, conversationState);

          if (!behaviourValidation.valid) {
            logger.warn('‚ö†Ô∏è Respuesta rechazada por behaviour', { errors: behaviourValidation.errors });

            if (attempt < maxRetries) {
              messages.push({
                role: 'system',
                content: `CORRECCI√ìN: ${behaviourValidation.errors.join(', ')}. Genera nueva respuesta corrigiendo estos errores.`,
              });
              continue;
            }
          }

          // Log m√©tricas
          orchestrationService.logMetrics(responseText, validation);

          logger.info('‚úÖ Respuesta generada y validada', {
            length: responseText.length,
            model: completion.model,
            tokensUsed: completion.usage.total_tokens,
            attempt,
            valid: validation.valid,
            warnings: validation.warnings.length,
          });

          return responseText;

        } catch (error) {
          lastError = error;

          // Verificar si es error retryable
          const isRetryable = error.status === 429 || // Rate limit
                              error.status === 503 || // Service unavailable
                              error.status === 500 || // Server error
                              error.code === 'ECONNRESET' ||
                              error.code === 'ETIMEDOUT';

          if (isRetryable && attempt < maxRetries) {
            const waitTime = attempt * 1000 + Math.random() * 500;
            logger.warn(`‚ö†Ô∏è OpenAI error (${error.status || error.code}). Retry ${attempt + 1}/${maxRetries}`);
            await new Promise(resolve => setTimeout(resolve, waitTime));
            continue;
          }

          throw error;
        }
      }

      // Si todo falla, usar fallback
      logger.error('‚ùå Todos los intentos fallaron, usando fallback');
      return orchestrationService.getFallbackResponse(lastError);

    } catch (error) {
      logger.error('Error generando respuesta:', {
        message: error.message,
        status: error.status,
        code: error.code,
      });

      // Fallback final
      return orchestrationService.getFallbackResponse(error);
    }
  }

  /**
   * üß† NUEVO: Genera respuesta CON THINKING ENGINE
   * El agente PIENSA antes de responder usando Chain-of-Thought
   *
   * @param {string} userMessage - √öltimo mensaje del usuario
   * @param {Array} conversationHistory - Historial
   * @param {Object} thinkingAnalysis - An√°lisis del Thinking Engine
   * @param {Object} leadScore - Calificaci√≥n del lead
   */
  async generateResponseWithThinking(userMessage, conversationHistory, thinkingAnalysis, leadScore) {
    try {
      logger.info('üß† Generando respuesta con Thinking Engine', {
        shopifyDetected: thinkingAnalysis.shopify.detected,
        painLevel: thinkingAnalysis.pain.level,
        recommendation: thinkingAnalysis.recommendation.action,
      });

      // Si debe descalificar, retornar mensaje directamente
      if (thinkingAnalysis.shopify.shouldDisqualify) {
        const platform = thinkingAnalysis.shopify.reason.includes('WooCommerce') ? 'WooCommerce' :
                         thinkingAnalysis.shopify.reason.includes('Magento') ? 'Magento' :
                         thinkingAnalysis.shopify.reason.includes('VTEX') ? 'VTEX' : 'otra plataforma';

        return `Datapify funciona solo con Shopify. Si alg√∫n d√≠a migras a Shopify, conversamos :)`;
      }

      // ============================================
      // CONSTRUIR CONTEXTO DE PENSAMIENTO
      // ============================================
      const thinkingContext = this.buildThinkingContext(thinkingAnalysis, conversationHistory);

      // ============================================
      // CHAIN-OF-THOUGHT PROMPT
      // El agente RAZONA antes de responder
      // ============================================
      const chainOfThoughtPrompt = `‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üß† AN√ÅLISIS DEL MENSAJE (Piensa antes de responder)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

${thinkingContext}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üí≠ AHORA RESPONDE:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Basado en el an√°lisis, responde de forma natural, estrat√©gica y humana.

IMPORTANTE:
- M√°ximo 2-3 l√≠neas
- 1 pregunta como m√°ximo
- Tono chileno casual
- Reconoce lo que el usuario acaba de decir
- Avanza la conversaci√≥n estrat√©gicamente`;

      // ============================================
      // CONSTRUIR MENSAJES PARA EL LLM
      // ============================================
      const messages = [
        {
          role: 'system',
          content: this.systemPrompt,
        },
        {
          role: 'system',
          content: chainOfThoughtPrompt,
        },
      ];

      // Agregar historial (√∫ltimos 6 mensajes)
      const recentHistory = conversationHistory.slice(-6);
      recentHistory.forEach(msg => {
        messages.push({
          role: msg.role === 'usuario' || msg.role === 'user' ? 'user' : 'assistant',
          content: msg.content,
        });
      });

      // Agregar mensaje actual
      messages.push({
        role: 'user',
        content: userMessage,
      });

      // ============================================
      // LLAMAR AL LLM CON PAR√ÅMETROS OPTIMIZADOS
      // ============================================
      const completion = await this.openai.chat.completions.create({
        model: 'gpt-4o',
        messages: messages,
        temperature: 0.7,  // üéØ AJUSTADO: M√°s consistente (antes: 0.9)
        max_tokens: 350,   // üéØ AJUSTADO: M√°s espacio para pensar (antes: 200)
        top_p: 0.95,
        frequency_penalty: 0.5,
        presence_penalty: 0.6,
      });

      let responseText = completion.choices[0].message.content.trim();

      // Formatear para WhatsApp
      const orchestrationService = require('./orchestrationService');
      responseText = orchestrationService.formatForWhatsApp(responseText);

      logger.info('‚úÖ Respuesta generada con Thinking Engine', {
        length: responseText.length,
        tokensUsed: completion.usage.total_tokens,
        shopifyAcknowledged: thinkingAnalysis.shopify.detected ? responseText.toLowerCase().includes('shopify') : 'n/a',
      });

      return responseText;

    } catch (error) {
      logger.error('Error generando respuesta con Thinking Engine:', error);

      // Fallback a m√©todo tradicional
      logger.warn('‚ö†Ô∏è Fallback a generateResponse tradicional');
      return this.generateResponse(userMessage, conversationHistory, leadScore);
    }
  }

  /**
   * üß† CONSTRUYE CONTEXTO DE PENSAMIENTO
   * Genera el prompt interno para que el agente "piense"
   */
  buildThinkingContext(analysis, history) {
    let context = '';

    // 1. ¬øQu√© detect√© en el √∫ltimo mensaje?
    context += 'üìù LO QUE ACABO DE DETECTAR:\n';

    if (analysis.shopify.detected) {
      context += `‚úÖ Usuario CONFIRM√ì SHOPIFY (confianza: ${(analysis.shopify.confidence * 100).toFixed(0)}%)\n`;
      context += `   M√©todo: ${analysis.shopify.method}\n`;
      context += `   ‚Üí IMPORTANTE: Reconoce esto en tu respuesta\n`;
    } else if (analysis.shopify.shouldDisqualify) {
      context += `‚ùå Usuario NO usa Shopify (${analysis.shopify.reason})\n`;
    } else {
      context += `‚ö†Ô∏è No confirm√≥ Shopify todav√≠a\n`;
    }

    if (analysis.pain.level !== 'none') {
      context += `üî• DOLOR DETECTADO: Nivel ${analysis.pain.level}\n`;
      context += `   Se√±ales: ${analysis.pain.signals.join(', ')}\n`;
      context += `   ‚Üí Empatiza con su problema\n`;
    }

    context += `üéØ Intenci√≥n: ${analysis.intent.primary}\n`;
    context += '\n';

    // 2. ¬øQu√© s√© del usuario?
    context += 'üë§ LO QUE S√â DEL USUARIO:\n';
    if (analysis.leadInfo.name) context += `- Nombre: ${analysis.leadInfo.name}\n`;
    if (analysis.leadInfo.business) context += `- Negocio: ${analysis.leadInfo.business}\n`;
    if (analysis.leadInfo.investsInAds) context += `- Invierte en publicidad\n`;

    context += `- Mensajes intercambiados: ${analysis.context.messageCount}\n`;
    context += `- Engagement: ${analysis.context.engagementLevel}\n`;
    context += `- Fase: ${analysis.context.phase}\n`;
    context += '\n';

    // 3. ¬øQu√© preguntas ya hice?
    const asked = analysis.context.questionsAsked;
    context += '‚ùì PREGUNTAS YA HECHAS:\n';
    if (asked.name) context += '- ‚úÖ Nombre\n';
    if (asked.platform) context += '- ‚úÖ Plataforma\n';
    if (asked.business) context += '- ‚úÖ Tipo de negocio\n';
    if (asked.pain) context += '- ‚úÖ Dolor/problema\n';
    if (asked.meeting) context += '- ‚úÖ Reuni√≥n propuesta\n';
    context += '\n';

    // 4. ¬øQu√© deber√≠a hacer ahora?
    context += 'üéØ RECOMENDACI√ìN ESTRAT√âGICA:\n';
    context += `Acci√≥n: ${analysis.recommendation.action}\n`;
    context += `Prioridad: ${analysis.recommendation.priority}\n`;
    context += `Raz√≥n: ${analysis.recommendation.reasoning}\n`;

    if (analysis.recommendation.nextQuestion) {
      context += `Sugerencia: "${analysis.recommendation.nextQuestion}"\n`;
    }

    return context;
  }

  /**
   * Califica un lead (igual que Gemini)
   */
  qualifyLead(conversationHistory) {
    if (!this.businessKnowledge) {
      return { temperature: 'cold', score: 0, signals: [], phase: 'APERTURA' };
    }

    const allMessages = conversationHistory.map(m => m.content.toLowerCase()).join(' ');

    // Contar se√±ales de cada tipo
    const hotSignals = this.businessKnowledge.lead_qualification.hot_lead_signals.filter(signal => {
      const keywords = this.extractKeywords(signal);
      return keywords.some(kw => allMessages.includes(kw.toLowerCase()));
    });

    const warmSignals = this.businessKnowledge.lead_qualification.warm_lead_signals.filter(signal => {
      const keywords = this.extractKeywords(signal);
      return keywords.some(kw => allMessages.includes(kw.toLowerCase()));
    });

    const coldSignals = this.businessKnowledge.lead_qualification.cold_lead_signals.filter(signal => {
      const keywords = this.extractKeywords(signal);
      return keywords.some(kw => allMessages.includes(kw.toLowerCase()));
    });

    // Calcular score
    let score = 0;
    score += hotSignals.length * 3;
    score += warmSignals.length * 2;
    score -= coldSignals.length * 1;
    score = Math.max(0, Math.min(10, score));

    // Determinar temperatura
    let temperature = 'cold';
    if (score >= 7 || hotSignals.length >= 2) {
      temperature = 'hot';
    } else if (score >= 4 || warmSignals.length >= 2) {
      temperature = 'warm';
    }

    // Determinar fase
    const messageCount = conversationHistory.length;
    let phase = 'APERTURA';
    if (messageCount >= 8) phase = 'CIERRE';
    else if (messageCount >= 5) phase = 'PRESENTACI√ìN DE VALOR';
    else if (messageCount >= 2) phase = 'DESCUBRIMIENTO';

    const signals = [...hotSignals, ...warmSignals];

    logger.info('üìä Lead calificado', {
      temperature,
      score,
      signals: signals.length,
      phase,
    });

    return {
      temperature,
      score,
      signals,
      phase,
      readyToSchedule: temperature === 'hot' && hotSignals.length >= 2,
    };
  }

  /**
   * Extrae keywords de una se√±al de calificaci√≥n
   */
  extractKeywords(signal) {
    const lowerSignal = signal.toLowerCase();
    const keywords = [];

    if (lowerSignal.includes('shopify')) keywords.push('shopify', 'tienda', 'ecommerce', 'e-commerce');
    if (lowerSignal.includes('vende') || lowerSignal.includes('ventas')) keywords.push('vendo', 'vendiendo', 'ventas', 'facturando', 'millones', 'palos', 'clp');
    if (lowerSignal.includes('publicidad')) keywords.push('publicidad', 'ads', 'meta', 'facebook', 'instagram', 'anuncios', 'campa√±as');
    if (lowerSignal.includes('agencia')) keywords.push('agencia', 'freelancer', 'tercerizado', 'contratar');
    if (lowerSignal.includes('cayeron') || lowerSignal.includes('irregula')) keywords.push('cayeron', 'ca√≠do', 'bajaron', 'irregular', 'fluct√∫an');
    if (lowerSignal.includes('frustraci√≥n')) keywords.push('frustrad', 'cansad', 'harto', 'no funciona', 'mal');
    if (lowerSignal.includes('precio')) keywords.push('precio', 'costo', 'cu√°nto', 'plan', 'pagar');
    if (lowerSignal.includes('n√∫meros')) keywords.push('ventas', 'millones', 'palos', 'clp', 'facturaci√≥n');
    if (lowerSignal.includes('identifica') && lowerSignal.includes('publicidad')) keywords.push('publicidad', 'ads', 'marketing', 'anuncios');
    if (lowerSignal.includes('comparte') && lowerSignal.includes('negocio')) keywords.push('mi tienda', 'mi negocio', 'vendo', 'tengo');
    if (lowerSignal.includes('agendar')) keywords.push('reuni√≥n', 'llamada', 'agendar', 'hablemos', 'tinca');

    return keywords;
  }

  /**
   * Limpia la respuesta (para compatibilidad)
   */
  cleanResponse(aiResponse) {
    return aiResponse.replace(/\[INTENT:SCHEDULE\][\s\S]*?\[\/INTENT\]/g, '').trim();
  }

  /**
   * Genera un resumen de reuni√≥n
   */
  async generateMeetingSummary(meetingData) {
    try {
      const prompt = `Genera un mensaje de confirmaci√≥n breve y profesional para una reuni√≥n agendada con estos datos:
- Nombre: ${meetingData.name}
- Motivo: ${meetingData.reason}
- Fecha: ${meetingData.date}
- Hora: ${meetingData.time}

Incluye un mensaje de bienvenida y confirma los detalles. M√°ximo 3 l√≠neas.`;

      const completion = await this.openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.7,
        max_tokens: 200,
      });

      return completion.choices[0].message.content;
    } catch (error) {
      logger.error('Error generando resumen de reuni√≥n:', error);
      return `‚úÖ Reuni√≥n agendada con √©xito para ${meetingData.date} a las ${meetingData.time}. Te esperamos, ${meetingData.name}.`;
    }
  }

  /**
   * FASE 2: Genera un resumen conciso de la conversaci√≥n para memoria persistente
   * M√°ximo 150 tokens para mantener costos bajos
   */
  async generateConversationSummary(messages, leadData) {
    try {
      // Formatear mensajes para el resumen
      const conversationText = messages
        .map(msg => `${msg.role === 'user' ? 'Cliente' : 'Vendedor'}: ${msg.content}`)
        .join('\n');

      const prompt = `Resume esta conversaci√≥n de ventas en m√°ximo 2-3 frases cortas. Enf√≥cate en:
1. ¬øQu√© busca/necesita el cliente?
2. Informaci√≥n clave del negocio (plataforma, ingresos, problemas)
3. Estado de la conversaci√≥n (interesado/descalificado/agend√≥)

Cliente: ${leadData?.name || 'Sin nombre'}
Negocio: ${leadData?.businessType || 'No especificado'}

Conversaci√≥n:
${conversationText}

Resumen (m√°ximo 50 palabras):`;

      const completion = await this.openai.chat.completions.create({
        model: 'gpt-4o-mini', // Modelo m√°s econ√≥mico
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3,
        max_tokens: 150,
      });

      const summary = completion.choices[0].message.content.trim();
      logger.info('üìù Resumen de conversaci√≥n generado', {
        messageCount: messages.length,
        summaryLength: summary.length,
      });

      return summary;
    } catch (error) {
      logger.error('Error generando resumen de conversaci√≥n:', error);
      // Fallback: resumen b√°sico sin IA
      return `Cliente ${leadData?.name || 'an√≥nimo'} - ${messages.length} mensajes intercambiados`;
    }
  }
}

module.exports = new OpenAIService();
